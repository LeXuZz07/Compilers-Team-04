import ply.lex as lex

# Reserved words
reserved_words = {
    'while': 'WHILE',
    'if': 'IF',
    'else': 'ELSE',
    'for': 'FOR',
    'do': 'DO',
    'break': 'BREAK',
    'int': 'INT',
    'float': 'FLOAT',
    'bool': 'BOOL',
    'null': 'NULL',
    'return': 'RETURN',
    'true': 'BOOL_TRUE',
    'false': 'BOOL_FALSE'
}

# Tokens
tokens = [
    'NUMBER', 'ID', 'LESS', 'LPAREN', 'RPAREN', 'EXPONENTIAL',
    'MORE', 'DIVISION', 'MULTIPLICATION', 'BRACKETS', 
    'OPERATORS', 'MODULE', 'EQUALS', 'SPOT',
    'SIMPLE_LITERAL', 'SPECIAL', 'LKEY', 'RKEY',
] + list(reserved_words.values())

# Definition of tokens
# Several tokens were modified for better data handling
# Individual operators such as the exponential are also assigned
t_LKEY = r'\{'
t_RKEY = r'\}'
t_LPAREN = r'\('
t_RPAREN = r'\)'
t_MULTIPLICATION = r'\*'
t_BRACKETS = r'[\[\]]'
t_OPERATORS = r'==|!=|<=|>=|<|>'
t_MODULE = r'%'
t_EQUALS = r'='
t_LESS = r'-'
t_MORE = r'\+'
t_EXPONENTIAL = r'\^'
t_DIVISION = r'/'
t_SPECIAL = r'[@#$%&_Â¿?]'
t_ignore = " \t"

# Defining specific tokens using functions

# Token representing numbers with decimal point
def t_SPOT(t):
    r'\d+\.\d+'
    t.value = float(t.value)# Converts the value to float type
    return t

# Token representing integers
def t_NUMBER(t):
    r'\d+'
    t.value = int(t.value)# Converts the value to integer type
    return t

# Token representing simple literals (in single quotes)
def t_SIMPLE_LITERAL(t):
    r"'([^'\\]|\\.)*'"
    return t

# Token representing identifiers (variable names, functions, etc.)
def t_ID(t):
    r'[a-zA-Z_][a-zA-Z_0-9]*'
    # If the identifier is a reserved word, change its type
    t.type = reserved_words.get(t.value, 'ID')
    return t

# Token to count new lines and update the lexer line number
def t_newline(t):
    r'\n+'
    t.lexer.lineno += t.value.count("\n")

# Error handling in case of illegal characters
def t_error(t):
    print("Illegal character '%s'" % t.value[0])# Reports illegal character
    t.lexer.skip(1)# Skip the character and continue parsing

# Building the lexer
lexer = lex.lex()



#giving an example for the lexical analyzer
# Lee el contenido del archivo en la variable ejemplo_entrada
with open("ejemplo.txt", "r") as file:
    ejemplo_entrada = file.read()  # Lee todo el contenido del archivo

#pass the input string to the lexer
lexer.input(ejemplo_entrada)

#Iterate over the tokens generated by the lexer
while True:
    token = lexer.token()
    if not token:
        break # When no more tokens are found, it exits
    print(token) 